# ğŸš€ AI Local Starter Kit

![AI Local Starter Kit Header](header.png)

[![ENGLISH](https://img.shields.io/badge/english-100000?style=for-the-badge&logo=languagetool&logoColor=white)](README.md)

> ğŸŒŸ Un entorno de desarrollo local completo para aplicaciones de inteligencia artificial, con mÃºltiples servicios preconfigurados para crear, gestionar y desplegar soluciones de IA.

## ğŸ“ DescripciÃ³n

AI Local Starter Kit proporciona un conjunto de herramientas y servicios de IA configurados para funcionar juntos en un entorno local mediante Docker Compose. Este kit facilita el desarrollo, prueba e implementaciÃ³n de aplicaciones de IA sin necesidad de depender exclusivamente de servicios en la nube.

## ğŸ§© Componentes

El kit incluye los siguientes servicios:

### ğŸ–¥ï¸ Interfaces y Herramientas de Desarrollo

- **ğŸ”„ Flowise** (Puerto 3001): Herramienta no-code/low-code para construir agentes de IA y flujos de trabajo.
- **ğŸŒ OpenWebUI** (Puerto 3000): Interfaz web para interactuar con modelos de IA.
- **âš™ï¸ n8n** (Puerto 5678): Plataforma de automatizaciÃ³n de flujos de trabajo.

### ğŸ§  Infraestructura de IA

- **ğŸ”Œ LiteLLM** (Puerto 4000): Proxy que simplifica la conexiÃ³n con diferentes modelos de IA.
- **ğŸ’» MCPO** (Puerto 8000): Proxy que permite acceder a servidores MCP a travÃ©s de una API REST estÃ¡ndar, haciendo que tus herramientas funcionen con agentes LLM y aplicaciones que esperan servidores OpenAPI.
- **ğŸ“„ Docling** (Puerto 5001): Sistema de gestiÃ³n de documentos optimizado para IA.
- **ğŸ”Š Kokoro-TTS** (Puerto 8880): Servicio de conversiÃ³n de texto a voz (Text-to-Speech) optimizado para funcionamiento local.

### ğŸ—„ï¸ Bases de Datos y Servicios de Soporte

- **ğŸ’¾ PostgreSQL para LiteLLM** (Puerto 5432): Base de datos para LiteLLM.
- **ğŸ’¾ PostgreSQL para n8n** (Puerto 5433): Base de datos para n8n.
- **ğŸ“Š Prometheus** (Puerto 9090): Sistema de monitoreo y alertas.
- **ğŸ”„ Watchtower**: Servicio para actualizaciones automÃ¡ticas de contenedores.

## ğŸ“‹ Requisitos

- ğŸ³ Docker y Docker Compose
- ğŸ“¥ Git (para clonar el repositorio)
- ğŸ’» Al menos 8GB de RAM disponible (recomendado 16GB+)
- ğŸ’½ Espacio en disco para los contenedores y volÃºmenes

## âš™ï¸ ConfiguraciÃ³n

### 1ï¸âƒ£ Clonar e Iniciar

1. Clone el repositorio:
   ```bash
   git clone https://mesquidar/ai-local-starter-kit.git
   cd ai-local-starter-kit
   ```

2. Edite el archivo `.env` con sus configuraciones personalizadas.

### 2ï¸âƒ£ ConfiguraciÃ³n de Modelos de IA

#### ğŸ”‘ Configurar API Keys en LiteLLM

Antes de iniciar los servicios, es **crucial** configurar las API Keys de los modelos de IA que desea utilizar. Existen dos formas de hacerlo:

1. **Desde la interfaz web**: Una vez que LiteLLM estÃ© en funcionamiento, puede aÃ±adir sus API keys directamente desde la interfaz web.

2. **Usando el archivo de configuraciÃ³n**: Edite el archivo `configs/litellm/litellm_config.yaml` para agregar sus claves API:
   ```yaml
   # Ejemplo para OpenAI
   model_list:
   - model_name: o3-mini
     litellm_params:
       model: openai/o3-mini
       api_key: sk-xxxxxxxxxxxxxxxxxxxxxxxx
 
   - model_name: gpt-4o
     litellm_params:
       model: openai/gpt-4o
       api_key: sk-xxxxxxxxxxxxxxxxxxxxxxxx
   
     # Agregar otros modelos segÃºn sea necesario
   ```

#### ğŸ”§ Configurar MCPO

El servicio MCPO fallarÃ¡ si no se configura correctamente:

1. Modifique el archivo `configs/mcpo/config.json` con la configuraciÃ³n adecuada para su hardware:
   ```json
   {
      "mcpServers": {
        "playwright": {
          "url": "http://192.168.1.2:12345/sse"
          ]
        }
      }
    }
   ```

#### ğŸ–¥ï¸ Utilizar Modelos Locales con LM Studio

Para utilizar modelos locales alojados en LM Studio:

1. Instale y configure [LM Studio](https://lmstudio.ai/) en su mÃ¡quina local
2. Inicie un servidor local en LM Studio con su modelo preferido
3. En OpenWebUI, agregue una nueva conexiÃ³n que apunte a LM Studio:
   - Vaya a ConfiguraciÃ³n > Conexiones
   - Agregue una nueva conexiÃ³n con:
     - Nombre: "LM Studio Local"
     - URL base: "http://localhost:1234/v1" (o el puerto que haya configurado)
     - API Key: SegÃºn la configuraciÃ³n en LM Studio

### 3ï¸âƒ£ Iniciar los Servicios

Una vez completada la configuraciÃ³n, inicie los servicios:

```bash
docker-compose -p AI-Local-Starter-Kit up -d
```

Para verificar que todos los servicios estÃ©n funcionando correctamente:

```bash
docker-compose ps
```

## ğŸšª Acceso a los Servicios

Una vez iniciados los servicios, puede acceder a ellos a travÃ©s de los siguientes URL:

- ğŸ”„ **Flowise**: [http://localhost:3001](http://localhost:3001)
- ğŸŒ **OpenWebUI**: [http://localhost:3000](http://localhost:3000)
- âš™ï¸ **n8n**: [http://localhost:5678](http://localhost:5678)
- ğŸ”Œ **LiteLLM API**: [http://localhost:4000](http://localhost:4000)
- ğŸ’» **MCPO**: [http://localhost:8000](http://localhost:8000)
- ğŸ“„ **Docling**: [http://localhost:5001](http://localhost:5001)
- ğŸ”Š **Kokoro-TTS**: [http://localhost:8880](http://localhost:8880)
- ğŸ“Š **Prometheus**: [http://localhost:9090](http://localhost:9090)

## ğŸ“‚ Estructura de Directorios

```
ai-local-starter-kit/
â”œâ”€â”€ configs/                # Configuraciones para los servicios
â”‚   â”œâ”€â”€ litellm/            # ConfiguraciÃ³n de LiteLLM
â”‚   â”œâ”€â”€ mcpo/               # ConfiguraciÃ³n de MCPO
â”‚   â””â”€â”€ openwebui/          # ConfiguraciÃ³n de OpenWebUI
â”œâ”€â”€ docker-compose.yml      # DefiniciÃ³n de servicios
â””â”€â”€ .env                    # Variables de entorno
```

## ğŸ› ï¸ PersonalizaciÃ³n

Para personalizar cualquiera de los servicios, puede modificar los archivos de configuraciÃ³n correspondientes en el directorio `configs/`.

## â“ SoluciÃ³n de Problemas

Si encuentra problemas con alguno de los servicios, puede revisar los logs con:

```bash
docker-compose logs [nombre-del-servicio]
```

### Problemas Comunes

1. **ğŸ”´ MCPO no inicia**: Verifique que la configuraciÃ³n del modelo sea correcta en `configs/mcpo/config.json`.
2. **ğŸ”´ LiteLLM no puede conectarse a modelos**: Confirme que las API keys sean correctas en la configuraciÃ³n.
3. **ğŸ”´ Falta de memoria**: Algunos modelos requieren mÃ¡s RAM. Ajuste la asignaciÃ³n de memoria en Docker Desktop.

## ğŸ‘¥ Contribuir

Las contribuciones son bienvenidas. Por favor, envÃ­e un Pull Request o abra un Issue para discutir los cambios propuestos.


