# ğŸš€ AI Local Starter Kit

![AI Local Starter Kit Header](header.png)

[![SPANISH](https://img.shields.io/badge/espaÃ±ol-100000?style=for-the-badge&logo=languagetool&logoColor=white)](README_ES.md)

> ğŸŒŸ A complete local development environment for artificial intelligence applications, with multiple preconfigured services to create, manage, and deploy AI solutions.

## ğŸ“ Description

AI Local Starter Kit provides a set of AI tools and services configured to work together in a local environment using Docker Compose. This kit facilitates the development, testing, and implementation of AI applications without the need to exclusively rely on cloud services.

## ğŸ§© Components

The kit includes the following services:

### ğŸ–¥ï¸ Interfaces and Development Tools

- **ğŸ”„ Flowise** (Port 3001): No-code/low-code tool for building AI agents and workflows.
- **ğŸŒ OpenWebUI** (Port 3000): Web interface for interacting with AI models.
- **âš™ï¸ n8n** (Port 5678): Workflow automation platform.

### ğŸ§  AI Infrastructure

- **ğŸ”Œ LiteLLM** (Port 4000): Proxy that simplifies connecting to different AI models.
- **ğŸ’» MCPO** (Port 8000):  A proxy that makes MCP servers accessible via standard RESTful OpenAPI, allowing your tools to work seamlessly with LLM agents and applications expecting OpenAPI servers.
- **ğŸ“„ Docling** (Port 5001): Document management system optimized for AI.
- **ğŸ”Š Kokoro-TTS** (Port 8880): Text-to-Speech service optimized for local operation.

### ğŸ—„ï¸ Databases and Support Services

- **ğŸ’¾ PostgreSQL for LiteLLM** (Port 5432): Database for LiteLLM.
- **ğŸ’¾ PostgreSQL for n8n** (Port 5433): Database for n8n.
- **ğŸ“Š Prometheus** (Port 9090): Monitoring and alerting system.
- **ğŸ”„ Watchtower**: Service for automatic container updates.

## ğŸ“‹ Requirements

- ğŸ³ Docker and Docker Compose
- ğŸ“¥ Git (to clone the repository)
- ğŸ’» At least 8GB of available RAM (16GB+ recommended)
- ğŸ’½ Disk space for containers and volumes

## âš™ï¸ Configuration

### 1ï¸âƒ£ Clone and Initialize

1. Clone the repository:
   ```bash
   git clone https://[repository-url]/ai-local-starter-kit.git
   cd ai-local-starter-kit
   ```

2. Edit the `.env` file with your custom configurations.

### 2ï¸âƒ£ AI Model Configuration

#### ğŸ”‘ Configure API Keys in LiteLLM

Before starting the services, it's **crucial** to configure the API Keys for the AI models you want to use. There are two ways to do this:

1. **From the web interface**: Once LiteLLM is running, you can add your API keys directly from the web interface.

2. **Using the configuration file**: Edit the `configs/litellm/litellm_config.yaml` file to add your API keys:
   ```yaml
   # Example for OpenAI
   model_list:
     - model_name: gpt-3.5-turbo
       litellm_params:
         model: openai/gpt-3.5-turbo
         api_key: sk-xxxxxxxxxxxxxxxxxxxxxxxx
   
     - model_name: gpt-4
       litellm_params:
         model: openai/gpt-4
         api_key: sk-xxxxxxxxxxxxxxxxxxxxxxxx
   
     # Add other models as needed
   ```

#### ğŸ”§ Configure MCPO

The MCPO service will fail if not properly configured:

1. Modify the `configs/mcpo/config.json` file with the appropriate configuration for your hardware:
   ```json
   {
     "model": "path/to/your/model",
     "n_gpu_layers": 32,
     "context_length": 4096,
     "temperature": 0.7
   }
   ```

#### ğŸ–¥ï¸ Use Local Models with LM Studio

To use local models hosted in LM Studio:

1. Install and configure [LM Studio](https://lmstudio.ai/) on your local machine
2. Start a local server in LM Studio with your preferred model
3. In OpenWebUI, add a new connection that points to LM Studio:
   - Go to Settings > Connections
   - Add a new connection with:
     - Name: "LM Studio Local"
     - Base URL: "http://localhost:1234/v1" (or the port you have configured)
     - API Key: According to the configuration in LM Studio

### 3ï¸âƒ£ Start the Services

Once the configuration is complete, start the services:

```bash
docker-compose -p AI Local Starter Kit up -d
```

To verify that all services are running correctly:

```bash
docker-compose ps
```

## ğŸšª Accessing the Services

Once the services are started, you can access them through the following URLs:

- ğŸ”„ **Flowise**: [http://localhost:3001](http://localhost:3001)
- ğŸŒ **OpenWebUI**: [http://localhost:3000](http://localhost:3000)
- âš™ï¸ **n8n**: [http://localhost:5678](http://localhost:5678)
- ğŸ”Œ **LiteLLM API**: [http://localhost:4000](http://localhost:4000)
- ğŸ’» **MCPO**: [http://localhost:8000](http://localhost:8000)
- ğŸ“„ **Docling**: [http://localhost:5001](http://localhost:5001)
- ğŸ”Š **Kokoro-TTS**: [http://localhost:8880](http://localhost:8880)
- ğŸ“Š **Prometheus**: [http://localhost:9090](http://localhost:9090)

## ğŸ“‚ Directory Structure

```
ai-local-starter-kit/
â”œâ”€â”€ configs/                # Configurations for services
â”‚   â”œâ”€â”€ litellm/            # LiteLLM configuration
â”‚   â”œâ”€â”€ mcpo/               # MCPO configuration
â”‚   â””â”€â”€ openwebui/          # OpenWebUI configuration
â”œâ”€â”€ docker-compose.yml      # Service definitions
â””â”€â”€ .env                    # Environment variables
```

## ğŸ› ï¸ Customization

To customize any of the services, you can modify the corresponding configuration files in the `configs/` directory.

## â“ Troubleshooting

If you encounter issues with any of the services, you can check the logs with:

```bash
docker-compose logs [service-name]
```

### Common Issues

1. **ğŸ”´ MCPO doesn't start**: Verify that the model configuration is correct in `configs/mcpo/config.json`.
2. **ğŸ”´ LiteLLM can't connect to models**: Confirm that the API keys are correct in the configuration.
3. **ğŸ”´ Lack of memory**: Some models require more RAM. Adjust memory allocation in Docker Desktop.

## ğŸ‘¥ Contributing

Contributions are welcome. Please send a Pull Request or open an Issue to discuss proposed changes.
